"""
Streamlit –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ –¥–ª—è –ø—Ä–æ–≥–Ω–æ–∑–∞ –ø—Ä–∏–±—ã–ª—å–Ω–æ—Å—Ç–∏ –¥–æ–±—ã—á–∏ –±–ª–æ–∫–∞
–ò—Å–ø–æ–ª—å–∑—É–µ—Ç –º–æ–¥–µ–ª—å Gradient Boosting Regressor
"""

import numpy as np
import pandas as pd
import streamlit as st

from sklearn.compose import ColumnTransformer
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.model_selection import train_test_split
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import OneHotEncoder, StandardScaler

# –ö–æ–Ω—Å—Ç–∞–Ω—Ç—ã
DATA_PATH = "mining_block_model.csv"
TARGET_COL = "Profit (USD)"


@st.cache_resource
def load_data_and_train_model():
    """
    –ó–∞–≥—Ä—É–∂–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ, –≤—ã–ø–æ–ª–Ω—è–µ—Ç –ø–æ–¥–≥–æ—Ç–æ–≤–∫—É –∏ –æ–±—É—á–∞–µ—Ç –º–æ–¥–µ–ª—å GradientBoosting.
    –†–µ–∑—É–ª—å—Ç–∞—Ç –∫—ç—à–∏—Ä—É–µ—Ç—Å—è –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è –ø–æ—Å–ª–µ–¥—É—é—â–∏—Ö –∑–∞–ø—É—Å–∫–æ–≤.
    """
    try:
        # –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
        df = pd.read_csv(DATA_PATH)
        
        if df.empty:
            raise ValueError("–ó–∞–≥—Ä—É–∂–µ–Ω–Ω—ã–π —Ñ–∞–π–ª –ø—É—Å—Ç")
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞–ª–∏—á–∏—è —Ü–µ–ª–µ–≤–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π
        if TARGET_COL not in df.columns:
            raise ValueError(f"–°—Ç–æ–ª–±–µ—Ü {TARGET_COL!r} –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ –¥–∞–Ω–Ω—ã—Ö. –î–æ—Å—Ç—É–ø–Ω—ã–µ —Å—Ç–æ–ª–±—Ü—ã: {list(df.columns)}")
        
        # –£–¥–∞–ª–µ–Ω–∏–µ —ç–∫—Å—Ç—Ä–µ–º–∞–ª—å–Ω—ã—Ö –≤—ã–±—Ä–æ—Å–æ–≤ –ø–æ –ø—Ä–∏–±—ã–ª–∏ (3 * IQR)
        profit_values = df[TARGET_COL].dropna()
        
        if len(profit_values) == 0:
            raise ValueError(f"–ù–µ—Ç –≤–∞–ª–∏–¥–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π –≤ —Å—Ç–æ–ª–±—Ü–µ {TARGET_COL}")
        
        q1 = profit_values.quantile(0.25)
        q3 = profit_values.quantile(0.75)
        iqr = q3 - q1
        
        if iqr == 0:
            st.warning("IQR —Ä–∞–≤–µ–Ω –Ω—É–ª—é, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—é –≤—ã–±—Ä–æ—Å–æ–≤")
            df_clean = df.copy()
        else:
            lower_extreme = q1 - 3 * iqr
            upper_extreme = q3 + 3 * iqr
            df_clean = df[
                (df[TARGET_COL] >= lower_extreme) & 
                (df[TARGET_COL] <= upper_extreme)
            ].reset_index(drop=True)
        
        if df_clean.empty:
            raise ValueError("–ü–æ—Å–ª–µ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ –≤—ã–±—Ä–æ—Å–æ–≤ –¥–∞—Ç–∞—Å–µ—Ç —Å—Ç–∞–ª –ø—É—Å—Ç—ã–º")
        
        # –ò—Å–∫–ª—é—á–µ–Ω–∏–µ —Å–ª—É–∂–µ–±–Ω—ã—Ö —Å—Ç–æ–ª–±—Ü–æ–≤
        drop_cols = []
        for col in ["Block_ID", "Target"]:
            if col in df_clean.columns:
                drop_cols.append(col)
        
        feature_cols = [c for c in df_clean.columns if c not in drop_cols + [TARGET_COL]]
        
        if not feature_cols:
            raise ValueError("–ù–µ –Ω–∞–π–¥–µ–Ω–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏")
        
        # –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ —á–∏—Å–ª–æ–≤—ã–µ –∏ –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
        numeric_features = [c for c in feature_cols if df_clean[c].dtype in ['int64', 'float64']]
        categorical_features = [c for c in feature_cols if df_clean[c].dtype == 'object']
        
        # –§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ –º–∞—Ç—Ä–∏—Ü –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –∏ —Ü–µ–ª–µ–≤–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π
        X = df_clean[feature_cols].copy()
        y = df_clean[TARGET_COL].copy()
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –ø—É—Å—Ç—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è
        if X.isnull().all().any():
            st.warning("–û–±–Ω–∞—Ä—É–∂–µ–Ω—ã —Å—Ç–æ–ª–±—Ü—ã –ø–æ–ª–Ω–æ—Å—Ç—å—é —Å–æ—Å—Ç–æ—è—â–∏–µ –∏–∑ NaN. –û–Ω–∏ –±—É–¥—É—Ç —É–¥–∞–ª–µ–Ω—ã.")
            X = X.dropna(axis=1, how='all')
            numeric_features = [c for c in numeric_features if c in X.columns]
            categorical_features = [c for c in categorical_features if c in X.columns]
            feature_cols = numeric_features + categorical_features
        
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ø—Ä–µ–ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–∞
        transformers = []
        
        if numeric_features:
            transformers.append(("num", StandardScaler(), numeric_features))
        
        if categorical_features:
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º sparse_output=False –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ —Å –Ω–æ–≤—ã–º–∏ –≤–µ—Ä—Å–∏—è–º–∏ sklearn
            # –î–ª—è —Å—Ç–∞—Ä—ã—Ö –≤–µ—Ä—Å–∏–π –º–æ–∂–Ω–æ —É–±—Ä–∞—Ç—å —ç—Ç–æ—Ç –ø–∞—Ä–∞–º–µ—Ç—Ä
            try:
                ohe = OneHotEncoder(drop="first", handle_unknown="ignore", sparse_output=False)
            except TypeError:
                # –î–ª—è —Å—Ç–∞—Ä—ã—Ö –≤–µ—Ä—Å–∏–π sklearn –∏—Å–ø–æ–ª—å–∑—É–µ–º sparse=False
                ohe = OneHotEncoder(drop="first", handle_unknown="ignore", sparse=False)
            transformers.append(("cat", ohe, categorical_features))
        
        if not transformers:
            raise ValueError("–ù–µ—Ç –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è (–Ω–∏ —á–∏—Å–ª–æ–≤—ã—Ö, –Ω–∏ –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö)")
        
        preprocessor = ColumnTransformer(transformers=transformers, remainder='drop')
        
        # –°–æ–∑–¥–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ GradientBoosting
        model = GradientBoostingRegressor(
            n_estimators=100,
            learning_rate=0.1,
            max_depth=5,
            random_state=42,
            verbose=0
        )
        
        # –°–æ–∑–¥–∞–Ω–∏–µ –ø–∞–π–ø–ª–∞–π–Ω–∞
        pipe = Pipeline(steps=[
            ("preprocess", preprocessor),
            ("model", model),
        ])
        
        # –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ train/test
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.2, random_state=42, shuffle=True
        )
        
        # –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
        with st.spinner("–û–±—É—á–∞–µ–º –º–æ–¥–µ–ª—å GradientBoosting (—ç—Ç–æ –º–æ–∂–µ—Ç –∑–∞–Ω—è—Ç—å 30-60 —Å–µ–∫—É–Ω–¥)..."):
            pipe.fit(X_train, y_train)
        
        # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
        y_train_pred = pipe.predict(X_train)
        y_test_pred = pipe.predict(X_test)
        
        # –ú–µ—Ç—Ä–∏–∫–∏
        metrics = {
            "rmse_train": float(np.sqrt(mean_squared_error(y_train, y_train_pred))),
            "r2_train": float(r2_score(y_train, y_train_pred)),
            "rmse_test": float(np.sqrt(mean_squared_error(y_test, y_test_pred))),
            "r2_test": float(r2_score(y_test, y_test_pred)),
        }
        
        # –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –ø–æ –ø—Ä–∏–∑–Ω–∞–∫–∞–º –¥–ª—è UI
        feature_info = {
            "feature_cols": feature_cols,
            "numeric_features": numeric_features,
            "categorical_features": categorical_features,
            "numeric_ranges": {},
            "categorical_values": {},
        }
        
        # –ó–∞–ø–æ–ª–Ω–µ–Ω–∏–µ –¥–∏–∞–ø–∞–∑–æ–Ω–æ–≤ –¥–ª—è —á–∏—Å–ª–æ–≤—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        for col in numeric_features:
            if col in X.columns:
                col_values = X[col].dropna()
                if len(col_values) > 0:
                    feature_info["numeric_ranges"][col] = {
                        "min": float(col_values.min()),
                        "max": float(col_values.max()),
                        "median": float(col_values.median()),
                    }
        
        # –ó–∞–ø–æ–ª–Ω–µ–Ω–∏–µ –∑–Ω–∞—á–µ–Ω–∏–π –¥–ª—è –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        for col in categorical_features:
            if col in X.columns:
                unique_vals = X[col].dropna().unique().tolist()
                if unique_vals:
                    feature_info["categorical_values"][col] = sorted(unique_vals)
        
        return pipe, metrics, feature_info
        
    except FileNotFoundError:
        st.error(f"–§–∞–π–ª {DATA_PATH} –Ω–µ –Ω–∞–π–¥–µ–Ω. –£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ —Ñ–∞–π–ª –Ω–∞—Ö–æ–¥–∏—Ç—Å—è –≤ —Ç–æ–π –∂–µ –ø–∞–ø–∫–µ, —á—Ç–æ –∏ streamlit_app.py")
        raise
    except Exception as e:
        st.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –¥–∞–Ω–Ω—ã—Ö –∏–ª–∏ –æ–±—É—á–µ–Ω–∏–∏ –º–æ–¥–µ–ª–∏: {str(e)}")
        raise


def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è Streamlit"""
    
    st.set_page_config(
        page_title="–ü—Ä–æ–≥–Ω–æ–∑ –ø—Ä–∏–±—ã–ª—å–Ω–æ—Å—Ç–∏ –±–ª–æ–∫–∞",
        page_icon="‚õèÔ∏è",
        layout="wide",
        initial_sidebar_state="expanded"
    )
    
    st.title("‚õèÔ∏è –ü—Ä–æ–≥–Ω–æ–∑ –ø—Ä–∏–±—ã–ª—å–Ω–æ—Å—Ç–∏ –¥–æ–±—ã—á–∏ –±–ª–æ–∫–∞")
    st.markdown("""
    –≠—Ç–æ—Ç —Å–µ—Ä–≤–∏—Å –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –º–æ–¥–µ–ª—å **Gradient Boosting Regressor** –¥–ª—è –ø—Ä–æ–≥–Ω–æ–∑–∞ –ø—Ä–∏–±—ã–ª–∏ 
    `Profit (USD)` –ø–æ –≥–µ–æ–ª–æ–≥–æ-—ç–∫–æ–Ω–æ–º–∏—á–µ—Å–∫–∏–º –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º –±–ª–æ–∫–∞.
    
    –ú–æ–¥–µ–ª—å –æ–±—É—á–µ–Ω–∞ –Ω–∞ –¥–∞–Ω–Ω—ã—Ö `mining_block_model.csv` —Å –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–æ–π:
    - –£–¥–∞–ª–µ–Ω–∏–µ —ç–∫—Å—Ç—Ä–µ–º–∞–ª—å–Ω—ã—Ö –≤—ã–±—Ä–æ—Å–æ–≤ (3 * IQR)
    - –ú–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ —á–∏—Å–ª–æ–≤—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
    - One-hot –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
    """)
    
    # –ó–∞–≥—Ä—É–∑–∫–∞ –∏ –æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
    try:
        model_pipeline, metrics, feature_info = load_data_and_train_model()
    except Exception as e:
        st.error("–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –º–æ–¥–µ–ª—å. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –ª–æ–≥–∏ –≤—ã—à–µ.")
        st.stop()
    
    # –û—Ç–æ–±—Ä–∞–∂–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫ –∫–∞—á–µ—Å—Ç–≤–∞ –º–æ–¥–µ–ª–∏
    st.subheader("üìä –ö–∞—á–µ—Å—Ç–≤–æ –º–æ–¥–µ–ª–∏")
    col1, col2, col3, col4 = st.columns(4)
    
    with col1:
        st.metric("Train RMSE", f"{metrics['rmse_train']:,.2f}")
    with col2:
        st.metric("Train R¬≤", f"{metrics['r2_train']:.4f}")
    with col3:
        st.metric("Test RMSE", f"{metrics['rmse_test']:,.2f}")
    with col4:
        st.metric("Test R¬≤", f"{metrics['r2_test']:.4f}")
    
    st.markdown("---")
    
    # –ò–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã–π –ø—Ä–æ–≥–Ω–æ–∑ –¥–ª—è –æ–¥–Ω–æ–≥–æ –±–ª–æ–∫–∞
    st.subheader("üîÆ –ò–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã–π –ø—Ä–æ–≥–Ω–æ–∑ –¥–ª—è –æ–¥–Ω–æ–≥–æ –±–ª–æ–∫–∞")
    
    numeric_features = feature_info["numeric_features"]
    categorical_features = feature_info["categorical_features"]
    
    st.sidebar.header("üìù –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –±–ª–æ–∫–∞")
    st.sidebar.markdown("–ó–∞–ø–æ–ª–Ω–∏—Ç–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –±–ª–æ–∫–∞ –¥–ª—è –ø—Ä–æ–≥–Ω–æ–∑–∞ –ø—Ä–∏–±—ã–ª–∏.")
    
    input_data = {}
    
    # –í–≤–æ–¥ —á–∏—Å–ª–æ–≤—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
    if numeric_features:
        st.sidebar.subheader("–ß–∏—Å–ª–æ–≤—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã")
        for col in numeric_features:
            if col in feature_info["numeric_ranges"]:
                r = feature_info["numeric_ranges"][col]
                span = r["max"] - r["min"]
                if span > 0:
                    min_val = r["min"] - 0.05 * span
                    max_val = r["max"] + 0.05 * span
                else:
                    min_val = r["min"] - 1
                    max_val = r["max"] + 1
                
                default_val = r["median"]
                
                input_data[col] = st.sidebar.number_input(
                    col,
                    value=float(default_val),
                    min_value=float(min_val),
                    max_value=float(max_val),
                    step=0.01 if r["max"] - r["min"] < 100 else 1.0,
                    help=f"–î–∏–∞–ø–∞–∑–æ–Ω: [{r['min']:.2f}, {r['max']:.2f}]"
                )
    
    # –í–≤–æ–¥ –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
    if categorical_features:
        st.sidebar.subheader("–ö–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã")
        for col in categorical_features:
            values = feature_info["categorical_values"].get(col, [])
            if values:
                default_index = 0
                input_data[col] = st.sidebar.selectbox(
                    col,
                    options=values,
                    index=default_index,
                    help=f"–î–æ—Å—Ç—É–ø–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è: {', '.join(values)}"
                )
    
    # –ö–Ω–æ–ø–∫–∞ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
    predict_button = st.sidebar.button("üöÄ –ü—Ä–µ–¥—Å–∫–∞–∑–∞—Ç—å –ø—Ä–∏–±—ã–ª—å", type="primary", use_container_width=True)
    
    if predict_button:
        try:
            # –§–æ—Ä–º–∏—Ä—É–µ–º DataFrame —Å –æ–¥–Ω–∏–º –Ω–∞–±–ª—é–¥–µ–Ω–∏–µ–º
            X_new = pd.DataFrame([input_data])
            
            # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ
            y_pred = model_pipeline.predict(X_new)[0]
            
            # –û—Ç–æ–±—Ä–∞–∂–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
            st.success("‚úÖ –ü—Ä–æ–≥–Ω–æ–∑ —É—Å–ø–µ—à–Ω–æ —Ä–∞—Å—Å—á–∏—Ç–∞–Ω!")
            
            col_pred1, col_pred2 = st.columns([1, 1])
            with col_pred1:
                st.metric(
                    "–ü—Ä–æ–≥–Ω–æ–∑–Ω–∞—è –ø—Ä–∏–±—ã–ª—å –±–ª–æ–∫–∞",
                    f"${y_pred:,.2f}",
                    delta=None
                )
            with col_pred2:
                if y_pred > 0:
                    st.success("üí∞ –ë–ª–æ–∫ –ø—Ä–∏–±—ã–ª—å–Ω—ã–π")
                else:
                    st.warning("‚ö†Ô∏è –ë–ª–æ–∫ —É–±—ã—Ç–æ—á–Ω—ã–π")
            
        except Exception as e:
            st.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–∏: {str(e)}")
    
    st.markdown("---")
    
    # –ü–∞–∫–µ—Ç–Ω–æ–µ –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏–µ
    st.subheader("üìÅ –ü–∞–∫–µ—Ç–Ω–æ–µ –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏–µ (CSV)")
    st.markdown("""
    –ó–∞–≥—Ä—É–∑–∏—Ç–µ CSV-—Ñ–∞–π–ª —Å –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏ –±–ª–æ–∫–æ–≤ (—Ç–µ –∂–µ —Å—Ç–æ–ª–±—Ü—ã, —á—Ç–æ –∏ –≤–æ –≤—Ö–æ–¥–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–∞—Ö).
    –°–µ—Ä–≤–∏—Å —Ä–∞—Å—Å—á–∏—Ç–∞–µ—Ç –ø—Ä–æ–≥–Ω–æ–∑ –ø—Ä–∏–±—ã–ª–∏ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –±–ª–æ–∫–∞.
    """)
    
    uploaded_file = st.file_uploader(
        "–ó–∞–≥—Ä—É–∑–∏—Ç–µ CSV —Ñ–∞–π–ª",
        type=["csv"],
        help="–§–∞–π–ª –¥–æ–ª–∂–µ–Ω —Å–æ–¥–µ—Ä–∂–∞—Ç—å –≤—Å–µ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ —Å—Ç–æ–ª–±—Ü—ã –ø—Ä–∏–∑–Ω–∞–∫–æ–≤"
    )
    
    if uploaded_file is not None:
        try:
            df_new = pd.read_csv(uploaded_file)
            
            if df_new.empty:
                st.error("–ó–∞–≥—Ä—É–∂–µ–Ω–Ω—ã–π —Ñ–∞–π–ª –ø—É—Å—Ç")
            else:
                feature_cols = feature_info["feature_cols"]
                missing_cols = [c for c in feature_cols if c not in df_new.columns]
                
                if missing_cols:
                    st.error(f"–í –∑–∞–≥—Ä—É–∂–µ–Ω–Ω–æ–º —Ñ–∞–π–ª–µ –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ —Å—Ç–æ–ª–±—Ü—ã: {', '.join(missing_cols)}")
                    st.info(f"–¢—Ä–µ–±—É–µ–º—ã–µ —Å—Ç–æ–ª–±—Ü—ã: {', '.join(feature_cols)}")
                else:
                    # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –¥–ª—è –≤—Å–µ—Ö —Å—Ç—Ä–æ–∫
                    with st.spinner("–†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º –ø—Ä–æ–≥–Ω–æ–∑—ã..."):
                        preds = model_pipeline.predict(df_new[feature_cols])
                    
                    # –§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
                    df_result = df_new.copy()
                    df_result["Predicted Profit (USD)"] = preds
                    
                    st.success(f"‚úÖ –ü—Ä–æ–≥–Ω–æ–∑ —Ä–∞—Å—Å—á–∏—Ç–∞–Ω –¥–ª—è {len(df_result)} –±–ª–æ–∫–æ–≤")
                    
                    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –ø—Ä–æ–≥–Ω–æ–∑–∞–º
                    col_stat1, col_stat2, col_stat3 = st.columns(3)
                    with col_stat1:
                        st.metric("–°—Ä–µ–¥–Ω—è—è –ø—Ä–∏–±—ã–ª—å", f"${df_result['Predicted Profit (USD)'].mean():,.2f}")
                    with col_stat2:
                        st.metric("–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –ø—Ä–∏–±—ã–ª—å", f"${df_result['Predicted Profit (USD)'].max():,.2f}")
                    with col_stat3:
                        profitable = (df_result['Predicted Profit (USD)'] > 0).sum()
                        st.metric("–ü—Ä–∏–±—ã–ª—å–Ω—ã—Ö –±–ª–æ–∫–æ–≤", f"{profitable} / {len(df_result)}")
                    
                    # –¢–∞–±–ª–∏—Ü–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
                    st.dataframe(
                        df_result.head(100),
                        use_container_width=True,
                        height=400
                    )
                    
                    if len(df_result) > 100:
                        st.info(f"–ü–æ–∫–∞–∑–∞–Ω—ã –ø–µ—Ä–≤—ã–µ 100 —Å—Ç—Ä–æ–∫ –∏–∑ {len(df_result)}")
                    
                    # –ö–Ω–æ–ø–∫–∞ —Å–∫–∞—á–∏–≤–∞–Ω–∏—è
                    csv_bytes = df_result.to_csv(index=False).encode("utf-8")
                    st.download_button(
                        "üíæ –°–∫–∞—á–∞—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ CSV",
                        data=csv_bytes,
                        file_name="predicted_profit.csv",
                        mime="text/csv",
                        use_container_width=True
                    )
        
        except pd.errors.EmptyDataError:
            st.error("–ó–∞–≥—Ä—É–∂–µ–Ω–Ω—ã–π CSV —Ñ–∞–π–ª –ø—É—Å—Ç –∏–ª–∏ –ø–æ–≤—Ä–µ–∂–¥–µ–Ω")
        except Exception as e:
            st.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ —Ñ–∞–π–ª–∞: {str(e)}")
    
    # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –º–æ–¥–µ–ª–∏ –≤ —Ñ—É—Ç–µ—Ä–µ
    st.markdown("---")
    with st.expander("‚ÑπÔ∏è –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –º–æ–¥–µ–ª–∏"):
        st.markdown(f"""
        **–ú–æ–¥–µ–ª—å**: Gradient Boosting Regressor
        
        **–ü–∞—Ä–∞–º–µ—Ç—Ä—ã –º–æ–¥–µ–ª–∏**:
        - –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–µ—Ä–µ–≤—å–µ–≤: 100
        - Learning rate: 0.1
        - –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –≥–ª—É–±–∏–Ω–∞: 5
        
        **–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤**:
        - –ß–∏—Å–ª–æ–≤—ã—Ö: {len(feature_info['numeric_features'])}
        - –ö–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö: {len(feature_info['categorical_features'])}
        - –í—Å–µ–≥–æ: {len(feature_info['feature_cols'])}
        
        **–†–∞–∑–º–µ—Ä –æ–±—É—á–∞—é—â–µ–π –≤—ã–±–æ—Ä–∫–∏**: ~{len(feature_info['feature_cols'])} –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–æ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
        """)


if __name__ == "__main__":
    main()
